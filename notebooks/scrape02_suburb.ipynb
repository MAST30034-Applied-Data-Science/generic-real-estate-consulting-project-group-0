{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all subrub\n",
    "\n",
    "import requests\n",
    "import html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from decimal import Decimal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request \n",
    "\n",
    "from random import uniform\n",
    "from random import sample\n",
    "\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from requests_ip_rotator import ApiGateway, EXTRA_REGIONS\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME FUNCTIONS USED IN ALL SCRAPING#\n",
    "\n",
    "#Notes:\n",
    "\n",
    "# (1)\n",
    "\n",
    "# If get responses are non-success, try to open the website, manually,\n",
    "# on your own browser with your own unblocked internet/ip-address.\n",
    "# Inspect the page to get the request header that works.\n",
    "# Copy the headers into the get_header function,\n",
    "# but leave out your user agent and keep the 'User-Agent':sample(ua_l,1)[0].\n",
    "\n",
    "# (2)\n",
    "\n",
    "# At the time of original scraping, All of the links that is crawled and obtained by scrape03_all_link.ipynb\n",
    "# was sucessfully scraped by scrape04_data.ipynb. However, for the past week, A few of the links seems to \n",
    "# not be retrievable, even when the request header was reset as adviced at (1).\n",
    "\n",
    "# However, the nature of the failures seems to be random, for example: out of 30 pages of Hawthorn rent listing,\n",
    "# 2 of the pages is not retrievable, it is believed that the error occured on the host's side.\n",
    "\n",
    "# (3)\n",
    "\n",
    "# Always couple get_gateway() with gateway.shutdown() \n",
    "\n",
    "# (4)\n",
    "\n",
    "# time.sleep(uniform(1,2)) is set after every request get in getpage_aws_ip() for ethical reason\n",
    "# the sleep values can be changed at your own discretion\n",
    "\n",
    "\n",
    "# Obtaining a big list of user agent to be rotated in the get headers\n",
    "def get_useragent():\n",
    "\n",
    "    file_list=['android-browser.csv','chrome.csv','firefox.csv','internet-explorer.csv','opera.csv','safari.csv']\n",
    "\n",
    "    ua_l = [];\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        url = 'https://raw.githubusercontent.com/N0taN3rd/userAgentLists/master/csv/'+file;\n",
    "        df = pd.read_csv(url, index_col=0)\n",
    "        ua_l += df.index.to_list();\n",
    "        \n",
    "    return ua_l;\n",
    "\n",
    "# Create effective, user-like, headers using user agent rotator\n",
    "def get_header(ua_l):\n",
    "    \n",
    "    headers = { \n",
    "               'If-Modified-Since': 'Tue, 27 Sep 2022 01:26:05 GMT',\n",
    "                'If-None-Match': \"a3P8L3LUf+cFKEYW/6EIEKfHggI=\",\n",
    "                'User-Agent':sample(ua_l,1)[0]}\n",
    "    \n",
    "    return headers;\n",
    "\n",
    "# Opening AmazonAPI gateways to \n",
    "def get_gateway(base_url):\n",
    "    \n",
    "    AWS_ACCESS_KEY_ID = 'AKIAUKKMLM7SJVLB377D'\n",
    "    AWS_SECRET_ACCESS_KEY = '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'\n",
    "    \n",
    "    gateway = ApiGateway(site=base_url, regions=EXTRA_REGIONS, access_key_id = AWS_ACCESS_KEY_ID, access_key_secret = AWS_SECRET_ACCESS_KEY)\n",
    "    gateway.start()\n",
    "    \n",
    "    return gateway\n",
    "\n",
    "def getpage_aws_ip(gateway,base_url,site,ua_l):\n",
    "    \n",
    "    page_not_found = True;\n",
    "    \n",
    "    count =1;\n",
    "    \n",
    "    headers = get_header(ua_l);\n",
    "    session = requests.Session()\n",
    "    session.mount(base_url, gateway)\n",
    "    response = session.post(base_url+site,headers=headers);\n",
    "    time.sleep(uniform(1,2))\n",
    "    \n",
    "    # Unqote the code below if you want to apply some retries with rotated IP Address.\n",
    "    # Change the maximum count at your own discretion\n",
    "    \n",
    "    '''\n",
    "    while response.ok == False and count <10:\n",
    "    \n",
    "        headers = get_header(ua_l);\n",
    "        session = requests.Session()\n",
    "        session.mount(base_url, gateway)\n",
    "        response = session.post(base_url+site,headers);\n",
    "        print(response)\n",
    "        time.sleep(uniform(1,2))\n",
    "        count =+1;\n",
    "    '''\n",
    "    \n",
    "    return response;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seed links (links to the first page of rent listings of each suburb)\n",
    "# to all suburbs in Victoria from pages scraped in scrape01;\n",
    "# see: https://www.oldlistings.com.au/site-map?state=VIC\n",
    "\n",
    "def get_suburb_link(base_url, sitemaplinks_l,ua_l):\n",
    "    \n",
    "    gateway = get_gateway(base_url);\n",
    "    \n",
    "    suburblink_l = [];\n",
    "    \n",
    "    for site_str in tqdm(sitemaplinks_l):\n",
    "        page = getpage_aws_ip(gateway,base_url,site_str,ua_l)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser');\n",
    "        td_tags = soup.find_all('td');\n",
    "    \n",
    "        for td in td_tags:\n",
    "            if td.find('a') != None:\n",
    "                link = td.find('a').get('href');\n",
    "                if ('rent' in link and 'show_streets' not in link):\n",
    "                    if link not in suburblink_l:\n",
    "                        suburblink_l.append(td.find('a').get('href'));\n",
    "    gateway.shutdown()\n",
    "    \n",
    "    return suburblink_l\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/seed.csv', newline='') as f:\n",
    "    data = list(csv.reader(f))\n",
    "    sitemaplinks_l = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.oldlistings.com.au';   \n",
    "\n",
    "ua_l = get_useragent();\n",
    "\n",
    "suburblink_l = get_suburb_link(base_url, sitemaplinks_l,ua_l);\n",
    "\n",
    "with open('../data/raw/suburblink.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL);\n",
    "    wr.writerow(suburblink_l)\n",
    "    \n",
    "with open('../data/raw/suburblink.csv', newline='') as f:\n",
    "    data = list(csv.reader(f))\n",
    "    data = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburblink_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
