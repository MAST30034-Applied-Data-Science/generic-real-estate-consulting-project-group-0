{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all subrub\n",
    "\n",
    "import requests\n",
    "import html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from decimal import Decimal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request \n",
    "\n",
    "from random import uniform\n",
    "from random import sample\n",
    "\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from requests_ip_rotator import ApiGateway, EXTRA_REGIONS\n",
    "import time\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME FUNCTIONS USED IN ALL SCRAPING#\n",
    "\n",
    "#Notes:\n",
    "\n",
    "# (1)\n",
    "\n",
    "# If get responses are non-success, try to open the website, manually,\n",
    "# on your own browser with your own unblocked internet/ip-address.\n",
    "# Inspect the page to get the request header that works.\n",
    "# Copy the headers into the get_header function,\n",
    "# but leave out your user agent and keep the 'User-Agent':sample(ua_l,1)[0].\n",
    "\n",
    "# (2)\n",
    "\n",
    "# At the time of original scraping, All of the links that is crawled and obtained by scrape03_all_link.ipynb\n",
    "# was sucessfully scraped by scrape04_data.ipynb. However, for the past week, A few of the links seems to \n",
    "# not be retrievable, even when the request header was reset as adviced at (1).\n",
    "\n",
    "# However, the nature of the failures seems to be random, for example: out of 30 pages of Hawthorn rent listing,\n",
    "# 2 of the pages is not retrievable, it is believed that the error occured on the host's side.\n",
    "\n",
    "# (3)\n",
    "\n",
    "# Always couple get_gateway() with gateway.shutdown() \n",
    "\n",
    "# (4)\n",
    "\n",
    "# time.sleep(uniform(1,2)) is set after every request get in getpage_aws_ip() for ethical reason\n",
    "# the sleep values can be changed at your own discretion\n",
    "\n",
    "\n",
    "# Obtaining a big list of user agent to be rotated in the get headers\n",
    "def get_useragent():\n",
    "\n",
    "    file_list=['android-browser.csv','chrome.csv','firefox.csv','internet-explorer.csv','opera.csv','safari.csv']\n",
    "\n",
    "    ua_l = [];\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        url = 'https://raw.githubusercontent.com/N0taN3rd/userAgentLists/master/csv/'+file;\n",
    "        df = pd.read_csv(url, index_col=0)\n",
    "        ua_l += df.index.to_list();\n",
    "        \n",
    "    return ua_l;\n",
    "\n",
    "# Create effective, user-like, headers using user agent rotator\n",
    "def get_header(ua_l):\n",
    "    \n",
    "    headers = { \n",
    "               'If-Modified-Since': 'Tue, 27 Sep 2022 01:26:05 GMT',\n",
    "                'If-None-Match': \"a3P8L3LUf+cFKEYW/6EIEKfHggI=\",\n",
    "                'User-Agent':sample(ua_l,1)[0]}\n",
    "    \n",
    "    return headers;\n",
    "\n",
    "# Opening AmazonAPI gateways to \n",
    "def get_gateway(base_url):\n",
    "    \n",
    "    AWS_ACCESS_KEY_ID = 'AKIAUKKMLM7SJVLB377D'\n",
    "    AWS_SECRET_ACCESS_KEY = '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'\n",
    "    \n",
    "    gateway = ApiGateway(site=base_url, regions=EXTRA_REGIONS, access_key_id = AWS_ACCESS_KEY_ID, access_key_secret = AWS_SECRET_ACCESS_KEY)\n",
    "    gateway.start()\n",
    "    \n",
    "    return gateway\n",
    "\n",
    "def getpage_aws_ip(gateway,base_url,site,ua_l):\n",
    "    \n",
    "    page_not_found = True;\n",
    "    \n",
    "    count =1;\n",
    "    \n",
    "    headers = get_header(ua_l);\n",
    "    session = requests.Session()\n",
    "    session.mount(base_url, gateway)\n",
    "    response = session.post(base_url+site,headers=headers);\n",
    "    time.sleep(uniform(1,2))\n",
    "    \n",
    "    # Unqote the code below if you want to apply some retries with rotated IP Address.\n",
    "    # Change the maximum count at your own discretion\n",
    "    '''\n",
    "    while response.ok == False and count <10:\n",
    "    \n",
    "        headers = get_header(ua_l);\n",
    "        session = requests.Session()\n",
    "        session.mount(base_url, gateway)\n",
    "        response = session.post(base_url+site,headers);\n",
    "        print(response)\n",
    "        time.sleep(uniform(1,2))\n",
    "        count =+1;\n",
    "    '''\n",
    "    \n",
    "    return response;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving functions\n",
    "\n",
    "def save_csv(path,columns,tpl):\n",
    "        \n",
    "    with open(path,'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(columns)\n",
    "        csv_out.writerow(tpl)\n",
    "        \n",
    "def update_csv(path,tpl):\n",
    "    with open(path,'a') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(tpl)\n",
    "            \n",
    "\n",
    "def save_list_json(unscrapped_l, path_str):\n",
    "    \n",
    "    with open(path_str, 'w') as myfile:\n",
    "        json.dump(unscrapped_l, myfile)\n",
    "        \n",
    "def get_list_json(path):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        l = json.load(f)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the pages for each suburb listings (growing the seed links in scrape02)\n",
    "\n",
    "# Visit the first page in each suburb once and collect all the links to all the pages for that suburb\n",
    "# Example, visit https://www.oldlistings.com.au/real-estate/VIC/Banksia+Peninsula/3875/rent/\n",
    "# to get : https://www.oldlistings.com.au/real-estate/VIC/Banksia+Peninsula/3875/rent/2\n",
    "# and https://www.oldlistings.com.au/real-estate/VIC/Banksia+Peninsula/3875/rent/3;\n",
    "\n",
    "def get_all_link(base_url,seeds_l,ua_l):\n",
    "    \n",
    "    gateway = get_gateway(base_url);\n",
    "    \n",
    "    \n",
    "    #for seeds_l in seeds_l_l:\n",
    "    \n",
    "    path = '../data/raw/all_links.json';\n",
    "    path_collected = '../data/raw/links_collected.json';\n",
    "    path_success = '../data/raw/links_success.json'\n",
    "    try:\n",
    "        all_links = get_list_json(path);\n",
    "        collected = get_list_json(path_collected);\n",
    "        success = get_list_json('path_success');\n",
    "    except:\n",
    "        all_links = [];\n",
    "        collected = [];\n",
    "        success  = [];\n",
    "    \n",
    "        \n",
    "    for seed_str in tqdm(seeds_l):\n",
    "        \n",
    "            if seed_str in collected:\n",
    "                continue;\n",
    "        \n",
    "            page = getpage_aws_ip(gateway,base_url,seed_str,ua_l)\n",
    "            \n",
    "            if page.ok == False:\n",
    "                collected.append(seed_str);\n",
    "                save_list_json(collected, path_collected)\n",
    "                continue;\n",
    "\n",
    "                \n",
    "            soup = BeautifulSoup(page.content, 'html.parser');\n",
    "                \n",
    "            #print(soup);\n",
    "            \n",
    "            li_tags = soup.find_all('li');\n",
    "            \n",
    "            start = False;\n",
    "    \n",
    "            for tag in li_tags:\n",
    "    \n",
    "                _a = tag.find('a') \n",
    "        \n",
    "                if _a != None:\n",
    "                    _link = _a.get('href');\n",
    "        \n",
    "                    if seed_str == _link:\n",
    "                        start = True;\n",
    "        \n",
    "                    if (seed_str in _link) and (start == True) and _link not in all_links:\n",
    "        \n",
    "                        all_links.append(_link)\n",
    "                        save_list_json(all_links, path)\n",
    "            \n",
    "            if seed_str not in all_links:\n",
    "                \n",
    "                all_links.append(seed_str)\n",
    "                save_list_json(all_links, path)\n",
    "                \n",
    "            collected.append(seed_str);\n",
    "            success.append(seed_str);\n",
    "                \n",
    "            save_list_json(collected, path_collected)\n",
    "            save_list_json(success, path_success)\n",
    "                \n",
    "    gateway.shutdown() \n",
    "            \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/suburblink.csv', newline='') as f:\n",
    "    data = list(csv.reader(f))\n",
    "    seeds_l = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API gateways in 17 regions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3511 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 17 endpoints with name 'https://www.oldlistings.com.au - IP Rotate API' (0 new).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 200/3511 [06:14<1:30:34,  1.64s/it]"
     ]
    }
   ],
   "source": [
    "ua_l = get_useragent();\n",
    "base_url = 'https://www.oldlistings.com.au'; \n",
    "all_links =get_all_link(base_url,seeds_l,ua_l);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
