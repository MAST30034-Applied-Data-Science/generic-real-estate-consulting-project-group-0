{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook does feature engineering from the pre-processed dataset\n",
    "# features are embeded by appending columns to each records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import geopy\n",
    "import collections\n",
    "import geopandas as gpd\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in the scraped and pre-processed past listing data\n",
    "files_dire = '../data/curated/'\n",
    "df = pd.read_csv(f'{files_dire}/processed_listing.csv').iloc[: , 1:]\n",
    "# sort the dataframe so the same property are followed with sorted list date\n",
    "df = df.sort_values(['address', 'list_date'], ascending=[True, False]).reset_index().iloc[:,1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# original size of the dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert the date to a date format as it cannot be auto recognised by Pandas\n",
    "df['list_date'] = df['list_date'].apply(pd.to_datetime)\n",
    "# remove the data that has not been listed in 2021 or 2022 again\n",
    "addresses = df.loc[df['list_date'].dt.year == 2022]['address'].tolist()\n",
    "addresses = df.loc[df['list_date'].dt.year == 2021]['address'].tolist() + addresses\n",
    "addresses = list(set(addresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of unique properties\n",
    "len(addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# size of the reduced dataframe\n",
    "df = df.loc[df['address'].isin(addresses)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# store the number of entries for the loop below\n",
    "n_rows = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# copy the first row of the dataframe to df2\n",
    "# df2 will only store each property per row to replace the prvious dataframe\n",
    "df2 = df.iloc[0:1,:]\n",
    "# listing history will be stored in a new column as a list with the format of [date difference in years, price difference]\n",
    "df2 = df2.assign(list_history = '')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialise for loop variables\n",
    "n_property = 0\n",
    "past_listing = []\n",
    "list_date = df.iloc[0]['list_date']\n",
    "list_price = df.iloc[0]['weekly_rent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run through all entries in the initial dataframe\n",
    "for i in tqdm(range(1, n_rows)):\n",
    "    # if the following row mataches with the previous one\n",
    "    # calculate the date and price difference\n",
    "    if df.iloc[i]['address'] == df2.iloc[n_property]['address']:\n",
    "        days = (df.iloc[i]['list_date'] - list_date)/ np.timedelta64(1, 'Y')\n",
    "        list_date = df.iloc[i]['list_date']\n",
    "        price = (list_price - df.iloc[i]['weekly_rent']) / df.iloc[i]['weekly_rent']\n",
    "        list_price = df.iloc[i]['weekly_rent']\n",
    "        if days != 0:\n",
    "            temp = [days, price]\n",
    "            past_listing.append(temp)\n",
    "    # if the address is different (different property)\n",
    "    # the past listings are stored\n",
    "    else:\n",
    "        df2.at[n_property, 'list_history'] = past_listing\n",
    "        n_property += 1\n",
    "        df2.loc[n_property] = df.iloc[i]\n",
    "        past_listing = []\n",
    "        list_date = df.iloc[i]['list_date']\n",
    "        list_price = df.iloc[i]['weekly_rent']\n",
    "# manually store the last record\n",
    "df2.at[n_property, 'list_history'] = past_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a new column to store the number of times that the property has been listed for lease\n",
    "df2['list_count'] = df2['list_history'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rename the column for legibility\n",
    "df2 = df2.rename(columns={'code': 'postcode'}, errors=\"coerce\")\n",
    "# this file is stored to share with other group members\n",
    "# df2.to_csv('../data/curated/processed_listing_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compare with the previous output to make sure we have roughly the expected amount of output\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pivot to the number of past listings\n",
    "col_list = (df2['list_count'].values.tolist())\n",
    "col_list.sort()\n",
    "counter = collections.Counter(col_list)\n",
    "print(counter)\n",
    "# it means there are 3xxxx properties that have been listed once in the past, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embed SA2 area information to the existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in the postcode information\n",
    "postcode_df = pd.read_csv('../data/raw/abs/australian_postcodes.csv')\n",
    "# only retain information that is relavent for faster running time\n",
    "postcode_df = postcode_df.loc[postcode_df['state'] == 'VIC']\n",
    "# rename columns to match dataframe so it can perform a 'vlookup' properly\n",
    "postcode_df['locality'] = postcode_df['locality'].str.title()\n",
    "postcode_df = postcode_df[['postcode', 'locality', 'SA2_NAME_2016', 'SA2_MAINCODE_2016', 'lgaregion']]\n",
    "postcode_df = postcode_df.rename(columns={'locality': 'suburb'}, errors=\"raise\")\n",
    "postcode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df3 will be the final output dataframe\n",
    "# the property informations are merged with its SA2 information by matching both postcode and suburb name\n",
    "df3 = pd.merge(df2, postcode_df, on=['postcode', 'suburb'])\n",
    "df3['SA2_MAINCODE_2016'] = df3['SA2_MAINCODE_2016'].astype(int)\n",
    "df3 = df3.rename(columns={'SA2_MAINCODE_2016': 'SA2'}, errors=\"raise\")\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metro_melb is a list of all LGAs of metropolitan melbourne\n",
    "# that is defined https://liveinmelbourne.vic.gov.au/discover/melbourne-victoria/metropolitan-melbourne\n",
    "metro_melb = ['Banyule', 'Bayside', 'Boroondara', 'Brimbank', 'Cardinia', 'Casey', 'Darebin', 'Frankston', 'Glen Eira', 'Greater Dandenong', 'Hobsons Bay', 'Hume', 'Kingston', 'Knox', 'Manningham', 'Maribyrnong', 'Maroondah', 'Melbourne', 'Melton', 'Monash', 'Moonee Valley', 'Moreland', 'Mornington Peninsula', 'Nillumbik', 'Port Phillip', 'Stonnington', 'Whitehorse', 'Whittlesea', 'Wyndham', 'Yarra', 'Yarra Ranges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# only retain entries that are in metro melbourne as they are more related with the study\n",
    "df3 = df3.loc[df3['lgaregion'].isin(metro_melb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of unique properties again\n",
    "len(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embed income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the csv file was exported from MS Excel as Python's ability to read csv with weird formating is limited\n",
    "income_df = pd.read_csv(f'{files_dire}/abs/income_distribution.csv')\n",
    "# only retain information that will be used\n",
    "income_df = income_df[['SA2', 'Earners', 'Median age of earners', 'Median', 'Mean', 'Top 10%']]\n",
    "income_df = income_df.replace(',','', regex=True)\n",
    "cols = ['Earners', 'Median age of earners', 'Median', 'Mean', 'Top 10%']\n",
    "income_df[cols] = income_df[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "income_df = income_df.rename(columns={'Earners': 'income_earner', 'Median age of earners': 'income_median_age', 'Median': 'income_median', 'Mean': 'income_mean', 'Top 10%': 'income_top_10_pct'}, errors=\"raise\")\n",
    "# information will be 'vlookup'ed by SA2 code, the data contains the number of earners in the SA2, median age of the earner\n",
    "# median income of the SA2, average income of the SA2 and how much of the population are ranked 10 percent within whole Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# merge income information\n",
    "df3 = pd.merge(df3, income_df, on='SA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embed population statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the csv file was exported from MS Excel as Python's ability to read csv with weird formating is limited\n",
    "population_df = pd.read_csv(f'{files_dire}/abs/population.csv')\n",
    "population_df = population_df.iloc[: , 8:]\n",
    "# age are grouped into following categories\n",
    "# they are sumed to produce these columns\n",
    "# 0-14 years (children), 15-24 years (early working age), 25-54 years (prime working age), 55-64 years (mature working age), 65 years and over (elderly)\n",
    "population_df['population_children'] = population_df[['0-4', '5–9', '10–14']].sum(axis = 1)\n",
    "population_df = population_df.drop(['0-4', '5–9', '10–14'], axis=1)\n",
    "population_df['population_prime_working'] = population_df[['25–29', '30–34', '35–39', '40–44', '45–49', '50–54']].sum(axis = 1)\n",
    "population_df = population_df.drop(['25–29', '30–34', '35–39', '40–44', '45–49', '50–54'], axis=1)\n",
    "population_df['population_elderly'] = population_df[['65–69', '70–74', '75–79', '80–84', '85 and over']].sum(axis = 1)\n",
    "population_df = population_df.drop(['65–69', '70–74', '75–79', '80–84', '85 and over'], axis=1)\n",
    "population_df['population_early_working'] = population_df.iloc[:,2] + population_df.iloc[:,3]\n",
    "population_df['population_mature_working'] = population_df.iloc[:,4] + population_df.iloc[:,5]\n",
    "population_df = population_df.drop(['15–19', '20–24', '55–59', '60–64', 'SA2 name'], axis=1)\n",
    "population_df = population_df.rename(columns={'Total persons': 'population_total', 'SA2 code': 'SA2'}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.merge(df3, population_df, on='SA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embed school locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "school_df = pd.read_csv('../data/raw/schools.csv')\n",
    "# drop useless columns to improve efficiency\n",
    "school_df = school_df.drop(['Entity_Type', 'School_No', 'School_Status', 'Address_Line_1', 'Address_Line_2',\n",
    "         'Address_Town', 'Address_State', 'Address_Postcode',\n",
    "         'Postal_Address_Line_1', 'Postal_Address_Line_2', 'Postal_Town',\n",
    "         'Postal_State', 'Postal_Postcode', 'Full_Phone_No', 'LGA_ID',\n",
    "         'LGA_Name'], axis=1)\n",
    "school_df['geo_coordinate'] = [(x, y) for x,y in zip(school_df['Y'], school_df['X'])]\n",
    "# only retain public schools\n",
    "school_df = school_df.loc[school_df['Education_Sector']=='Government']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# this is adapted from stackoverflow\n",
    "# https://codereview.stackexchange.com/questions/28207/finding-the-closest-point-to-a-list-of-points\n",
    "# in the future, we can potentially find 3-5 closest schools and run API find find the truly closest one\n",
    "# sometimes proximity is not equivalent to closest \n",
    "# e.g. 1.5km and 1.7km, but driving to the latter one is faster\n",
    "# this thought also applies to hospitals, train stations\n",
    "# it is a result of limited usuage of API\n",
    "def closest_point(point, points):\n",
    "    # Find closest point from a list of points.\n",
    "    return points[cdist([point], points).argmin()]\n",
    "def match_value(df, col1, x, col2):\n",
    "    # Match value x from col1 row to value in col2.\n",
    "    return df[df[col1] == x][col2].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3['geo_coordinate'] = [(x, y) for x,y in zip(df3['lat'], df3['lon'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### primary schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "school_type = ['Primary', 'Pri/Sec']\n",
    "school_primary_df = school_df.loc[school_df['School_Type'].isin(school_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# primary school\n",
    "df3['closest_primary_school_loc'] = tqdm([closest_point(x, list(school_primary_df['geo_coordinate'])) for x in df3['geo_coordinate']])\n",
    "df3['primary_school_name'] = tqdm([match_value(school_primary_df, 'geo_coordinate', x, 'School_Name') for x in df3['closest_primary_school_loc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### secondary schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "school_type = ['Secondary', 'Pri/Sec']\n",
    "school_secondary_df = school_df.loc[school_df['School_Type'].isin(school_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# embed secondary school ranking information\n",
    "# considering some family may rent a property for its free education\n",
    "school_ranking = pd.read_html('https://bettereducation.com.au/Results/vcePublicSchoolResults.aspx')[-1]\n",
    "school_ranking = school_ranking[['Better Education Rank', 'School', 'Unit 3-4 cohort']]\n",
    "school_ranking = school_ranking.rename(columns={'School': 'School_Name', 'Better Education Rank': 'secondary_school_rank', 'Unit 3-4 cohort': 'secondary_cohort'}, errors=\"coerce\")\n",
    "school_secondary_df = pd.merge(school_secondary_df, school_ranking, on='School_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3['closest_secondary_school_loc'] = tqdm([closest_point(x, list(school_secondary_df['geo_coordinate'])) for x in df3['geo_coordinate']])\n",
    "df3['secondary_school_name'] = tqdm([match_value(school_secondary_df, 'geo_coordinate', x, 'School_Name') for x in df3['closest_secondary_school_loc']])\n",
    "df3['secondary_school_rank'] = tqdm([match_value(school_secondary_df, 'geo_coordinate', x, 'secondary_school_rank') for x in df3['closest_secondary_school_loc']])\n",
    "df3['secondary_school_cohort'] = tqdm([match_value(school_secondary_df, 'geo_coordinate', x, 'secondary_cohort') for x in df3['closest_secondary_school_loc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# geocode are stored in the format of '(***.*****, ***.*****)'\n",
    "# we need to split them for the direction API used later\n",
    "# split them by the comma in the middle\n",
    "df3['closest_primary_school_loc'] = df3['closest_primary_school_loc'].astype('str')\n",
    "df3['closest_secondary_school_loc'] = df3['closest_secondary_school_loc'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3[['pri_lat', 'pri_lon']] = df3['closest_primary_school_loc'].str.split(', ', 1, expand=True)\n",
    "df3[['sec_lat', 'sec_lon']] = df3['closest_secondary_school_loc'].str.split(',', 1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove bracket and turn lat/lon into floats\n",
    "bracket_remove = ['pri_lat', 'pri_lon', 'sec_lat', 'sec_lon']\n",
    "for i in bracket_remove:\n",
    "    df3[i] = df3[i].str.strip('()')\n",
    "df3[bracket_remove] = df3[bracket_remove].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df3.drop(['closest_primary_school_loc', 'closest_secondary_school_loc'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### embed population projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "population_projection_df = pd.read_csv('../data/raw/population_projection.csv')\n",
    "population_projection_df = population_projection_df.loc[population_projection_df['SEX'] == 'Persons']\n",
    "population_projection_df = population_projection_df.loc[population_projection_df['YEAR'] == 2027]\n",
    "population_projection_df = population_projection_df.drop(['YEAR', 'SA2_NAME', 'SEX'], axis=1)\n",
    "# they are categories as well with the previous population statistics\n",
    "# 0-14 years (children), 15-24 years (early working age), 25-54 years (prime working age), 55-64 years (mature working age), 65 years and over (elderly)\n",
    "population_projection_df['proj_population_children'] = population_projection_df[['Age0-4', 'Age5-9', 'Age10-14']].sum(axis = 1)\n",
    "population_projection_df = population_projection_df.drop(['Age0-4', 'Age5-9', 'Age10-14'], axis=1)\n",
    "population_projection_df['proj_population_early_working'] = population_projection_df[['Age15-19', 'Age20-24']].sum(axis = 1)\n",
    "population_projection_df = population_projection_df.drop(['Age15-19', 'Age20-24'], axis=1)\n",
    "population_projection_df['proj_population_prime_working'] = population_projection_df[['Age25-29', 'Age30-34', 'Age35-39', 'Age40-44', 'Age45-49', 'Age50-54']].sum(axis = 1)\n",
    "population_projection_df = population_projection_df.drop(['Age25-29', 'Age30-34', 'Age35-39', 'Age40-44', 'Age45-49', 'Age50-54'], axis=1)\n",
    "population_projection_df['proj_population_mature_working'] = population_projection_df[['Age55-59', 'Age60-64']].sum(axis = 1)\n",
    "population_projection_df = population_projection_df.drop(['Age55-59', 'Age60-64'], axis=1)\n",
    "population_projection_df['proj_population_elderly'] = population_projection_df[['Age65-69', 'Age70-74', 'Age75-79', 'Age80-84', 'Age85+']].sum(axis = 1)\n",
    "population_projection_df = population_projection_df.drop(['Age65-69', 'Age70-74', 'Age75-79', 'Age80-84', 'Age85+'], axis=1)\n",
    "population_projection_df = population_projection_df.rename(columns={'Total': 'proj_population_total', 'SA2_CODE': 'SA2'}, errors=\"raise\")\n",
    "population_projection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.merge(df3, population_projection_df, on='SA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hospitals_df = pd.read_csv('../data/raw/hospitals.csv')\n",
    "hospitals_df = hospitals_df.loc[hospitals_df['Emergency Capable'] == 'YES']\n",
    "hospitals_df['full_address'] = hospitals_df['Location Address'] + ', ' +hospitals_df['Suburb'] + ', Victoria'\n",
    "hospitals_df = hospitals_df[['Formal Name', 'full_address']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the geocode of each hospital\n",
    "# it is highly similar to schools \n",
    "temp = []\n",
    "for i in tqdm(hospitals_df['full_address']):\n",
    "    locator = geopy.Nominatim(user_agent=\"myGeocoder\");\n",
    "    location = locator.geocode(i,timeout=None);\n",
    "    if location != None:\n",
    "        info = [i, location.latitude, location.longitude]\n",
    "        temp.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hospital_geo = pd.DataFrame(temp, columns=['full_address', 'lat', 'lon'])\n",
    "hospitals_df = pd.merge(hospitals_df, hospital_geo, on=['full_address'])\n",
    "hospitals_df = hospitals_df.drop('full_address', axis=1)\n",
    "hospitals_df['geo_coordinate'] = [(x, y) for x,y in zip(hospitals_df['lat'], hospitals_df['lon'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3['closest_ed_loc'] = tqdm([closest_point(x, list(hospitals_df['geo_coordinate'])) for x in df3['geo_coordinate']])\n",
    "df3['closest_ed_name'] = tqdm([match_value(hospitals_df, 'geo_coordinate', x, 'Formal Name') for x in df3['closest_ed_loc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3['closest_ed_loc'] = df3['closest_ed_loc'].astype('str')\n",
    "df3[['ed_lat', 'ed_lon']] = df3['closest_ed_loc'].str.split(', ', 1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bracket_remove = ['ed_lat', 'ed_lon']\n",
    "for i in bracket_remove:\n",
    "    df3[i] = df3[i].str.strip('()')\n",
    "df3[bracket_remove] = df3[bracket_remove].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df3.drop('closest_ed_loc', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### train station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in the shape file\n",
    "train_df = gpd.read_file('../data/raw/PTV/PTV_METRO_TRAIN_STATION.shp')\n",
    "# get latitude and longtitude of each train station\n",
    "train_df = train_df[['STOP_NAME', 'LATITUDE', 'LONGITUDE', 'ROUTEUSSP']]\n",
    "train_df['geo_coordinate'] = [(x, y) for x, y in zip(train_df['LATITUDE'], train_df['LONGITUDE'])]\n",
    "train_df['n_routes'] = train_df['ROUTEUSSP'].str.count(',') + 1\n",
    "train_df = train_df.drop(['LONGITUDE', 'LATITUDE', 'ROUTEUSSP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# embed train station information\n",
    "# very similar to schools and hospitals above\n",
    "df3['closest_train_loc'] = tqdm([closest_point(x, list(train_df['geo_coordinate'])) for x in df3['geo_coordinate']])\n",
    "df3['train_stop'] = tqdm([match_value(train_df, 'geo_coordinate', x, 'STOP_NAME') for x in df3['closest_train_loc']])\n",
    "df3['train_n_lines'] = tqdm([match_value(train_df, 'geo_coordinate', x, 'n_routes') for x in df3['closest_train_loc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3['closest_train_loc'] = df3['closest_train_loc'].astype('str')\n",
    "df3[['train_lat', 'train_lon']] = df3['closest_train_loc'].str.split(', ', 1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bracket_remove = ['train_lat', 'train_lon']\n",
    "for i in bracket_remove:\n",
    "    df3[i] = df3[i].str.strip('()')\n",
    "df3[bracket_remove] = df3[bracket_remove].apply(pd.to_numeric, errors='coerce')\n",
    "df3 = df3.drop('closest_train_loc', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### final check and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df3.drop('geo_coordinate', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3.isna().sum()\n",
    "# consider we still have a lot of entries\n",
    "# and we want to produce the best model\n",
    "# drop all rows with any NaN\n",
    "# this shall not be applied if the dataset is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3['type'] =  df['type'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df3[df3.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# there were some geocode that are obvisouly outside of Victoria\n",
    "# check if there is any\n",
    "features = ['lat', 'lon', 'pri_lat', 'pri_lon', 'sec_lat', 'sec_lon', 'ed_lat', 'ed_lon', 'train_lat', 'train_lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# these numbers are roughly the boundries\n",
    "# but it is good enough to remove the ones that are obvioulsy unreasonable\n",
    "# shape file could be potentially used if the project is highly rigourous\n",
    "df3 = df3[df3['lat'] < -34]\n",
    "df3 = df3[df3['lat'] > -40]\n",
    "df3 = df3[df3['lon'] > 140]\n",
    "df3 = df3[df3['lon'] < 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the latitude and longtitude for the embeded features \n",
    "for i in features:\n",
    "    print(i)\n",
    "    print(df3[i].min())\n",
    "    print(df3[i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('../data/curated/listing_with_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
