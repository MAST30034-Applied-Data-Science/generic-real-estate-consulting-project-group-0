{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping from domain.com.au\n",
    "Anthony He 1133985\n",
    "\n",
    "This part is adapted from the sample code provided"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/nhe/opt/anaconda3/lib/python3.9/site-packages (4.62.3)\r\n"
     ]
    }
   ],
   "source": [
    "# import packages (sorted alphabetically)\n",
    "!pip install tqdm # please skip this line if it is already installed\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from random import random\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# all files will be stored in the property_meta folder\n",
    "property_files = '../data/raw/property_meta'\n",
    "if not os.path.exists(property_files):\n",
    "    os.makedirs(property_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# read the scraped url\n",
    "# if the url has been scraped, then it will not be scraped again\n",
    "scraped_url = []\n",
    "if os.path.exists(f'{property_files}/property_url.csv'):\n",
    "    with open(f'{property_files}/property_url.csv', newline='') as inputfile:\n",
    "        for row in csv.reader(inputfile):\n",
    "            scraped_url.append(row[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set the header of the soup\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppelWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "sort_methods = [\"default-desc\", \"dateupdated-desc\", \"price-asc\", \"price-desc\", \"suburb-asc\"]\n",
    "N_PAGES = range(1, 51) # update this to your liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialise varaiables\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/y5/myhyydfd6h9dlwv1qmqtqhmr0000gn/T/ipykernel_91942/3501883684.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mpage\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mN_PAGES\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0murl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBASE_URL\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\"/rent/melbourne-region-vic/?sort={sort_method}&page={page}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m         \u001B[0mbs_object\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBeautifulSoup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"html.parser\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m         \u001B[0;31m# find the unordered list (ul) elements which are the results, then\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0;31m# find all href (a) tags that are from the base_url website.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    361\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 362\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_feed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    363\u001B[0m                 \u001B[0msuccess\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    364\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py\u001B[0m in \u001B[0;36m_feed\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    446\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 448\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmarkup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    449\u001B[0m         \u001B[0;31m# Close out any unfinished strings and close all the open tags.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mendData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/_htmlparser.py\u001B[0m in \u001B[0;36mfeed\u001B[0;34m(self, markup)\u001B[0m\n\u001B[1;32m    390\u001B[0m         \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msoup\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msoup\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 392\u001B[0;31m             \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmarkup\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    393\u001B[0m             \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    394\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mHTMLParseError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/html/parser.py\u001B[0m in \u001B[0;36mfeed\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    108\u001B[0m         \"\"\"\n\u001B[1;32m    109\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrawdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrawdata\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgoahead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/html/parser.py\u001B[0m in \u001B[0;36mgoahead\u001B[0;34m(self, end)\u001B[0m\n\u001B[1;32m    168\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'<'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mstarttagopen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrawdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# < + letter\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 170\u001B[0;31m                     \u001B[0mk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse_starttag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    171\u001B[0m                 \u001B[0;32melif\u001B[0m \u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"</\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m                     \u001B[0mk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse_endtag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/html/parser.py\u001B[0m in \u001B[0;36mparse_starttag\u001B[0;34m(self, i)\u001B[0m\n\u001B[1;32m    342\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandle_startendtag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    343\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 344\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandle_starttag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    345\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtag\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCDATA_CONTENT_ELEMENTS\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    346\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_cdata_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/_htmlparser.py\u001B[0m in \u001B[0;36mhandle_starttag\u001B[0;34m(self, name, attrs, handle_empty_element)\u001B[0m\n\u001B[1;32m    149\u001B[0m         \u001B[0;31m#print(\"START\", name)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m         \u001B[0msourceline\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msourcepos\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetpos\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m         tag = self.soup.handle_starttag(\n\u001B[0m\u001B[1;32m    152\u001B[0m             \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattr_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msourceline\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msourceline\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m             \u001B[0msourcepos\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msourcepos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py\u001B[0m in \u001B[0;36mhandle_starttag\u001B[0;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos)\u001B[0m\n\u001B[1;32m    724\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_most_recent_element\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnext_element\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtag\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    725\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_most_recent_element\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtag\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 726\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpushTag\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    727\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtag\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    728\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py\u001B[0m in \u001B[0;36mpushTag\u001B[0;34m(self, tag)\u001B[0m\n\u001B[1;32m    546\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagStack\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    547\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcurrentTag\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagStack\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 548\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mtag\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mROOT_TAG_NAME\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    549\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen_tag_counter\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtag\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    550\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtag\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreserve_whitespace_tags\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# gather all links that should be scraped\n",
    "for sort_method in tqdm(sort_methods):\n",
    "    for page in N_PAGES:\n",
    "        url = BASE_URL + f\"/rent/melbourne-region-vic/?sort={sort_method}&page={page}\"\n",
    "        bs_object = BeautifulSoup(requests.get(url, headers = headers).text, \"html.parser\")\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "        index_links = bs_object.find(\"ul\",{\"data-testid\": \"results\"}).findAll(\"a\",href=re.compile(f\"{BASE_URL}/*\"))\n",
    "        for link in index_links:\n",
    "            # if its a property address, add it to the list\n",
    "            if 'address' in link['class']:\n",
    "                url_links.append(link['href'])\n",
    "        sleep(round(random(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of urls\n",
    "len(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only retains the ones that were not scraped\n",
    "temp = []\n",
    "for i in url_links:\n",
    "    if not i in scraped_url:\n",
    "        temp.append(i)\n",
    "url_links = temp\n",
    "# check the number of urls that is going to be scraped in this run\n",
    "len(url_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url_links = list(set(url_links))\n",
    "num_url = len(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# urls are stored in a csv file\n",
    "# as property information are updated real time and scraped over a week\n",
    "# future scraping should be compared to this to avoid duplication\n",
    "with open(f'{property_files}/property_url.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for x in url_links + scraped_url:\n",
    "        csvwriter.writerow([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for each url, scrape some basic metadata\n",
    "# this segment of code may need to be modified if domain.com.au makes any changes\n",
    "# this code is valid at the time of 12 September\n",
    "for property_url in tqdm(url_links):\n",
    "    bs_object = BeautifulSoup(requests.get(property_url, headers = headers).text, \"html.parser\")\n",
    "    # looks for the header class to get property name\n",
    "    property_metadata[property_url]['name'] = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text\n",
    "    property_metadata[property_url]['type'] = bs_object.find(\"div\", {\"data-testid\": \"listing-summary-property-type\"}).text\n",
    "    # looks for the div containing a summary title for cost\n",
    "    property_metadata[property_url]['cost_text'] = bs_object.find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}).text\n",
    "    # extract coordinates from the hyperlink provided\n",
    "    property_metadata[property_url]['coordinates'] = [\n",
    "        float(coord) for coord in re.findall(\n",
    "            r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "            bs_object\n",
    "                .find(\n",
    "                \"a\",\n",
    "                {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "            )\n",
    "                .attrs['href']\n",
    "        )[0].split(',')\n",
    "    ]\n",
    "    property_metadata[property_url]['rooms'] = [\n",
    "        re.findall(r'\\d\\s[A-Za-z]+', feature.text) for feature in bs_object\n",
    "            .find(\"div\", {\"data-testid\": \"property-features\"})\n",
    "            .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "    ]\n",
    "    property_metadata[property_url]['desc'] = re.sub(r'<br\\/>', '\\n', str(bs_object.find(\"p\"))).strip('</p>')\n",
    "    sleep(round(3*random(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the results of scraping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# merge json file with the data scraped this time\n",
    "if os.path.exists(f'{property_files}/property_metadata.json'):\n",
    "    with open(f'{property_files}/property_metadata.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    d = defaultdict(list, data)\n",
    "    for key, value in d.items():\n",
    "        for subkey, subvalue in value.items():\n",
    "            property_metadata[key][subkey] = subvalue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# write the new json file\n",
    "with open(f'{property_files}/property_metadata.json', 'w') as f:\n",
    "    json.dump(property_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# merge csv file with the data scraped this time\n",
    "if os.path.exists(f'{property_files}/property_metadata.csv'):\n",
    "    df = pd.read_csv(f'{property_files}/property_metadata.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# write the current scraped metadata into a Pandas dataframe to save them into a csv file\n",
    "df2 = pd.DataFrame(property_metadata).T.reset_index()\n",
    "df2 = df2.rename(columns = {'index':'url'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "if os.path.exists(f'{property_files}/property_metadata.csv'):\n",
    "    save_csv = pd.concat([df, df2])\n",
    "else:\n",
    "    save_csv = df2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_csv.to_csv(f'{property_files}/property_metadata.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}