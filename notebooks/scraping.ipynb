{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scraping from domain.com.au\n",
    "Anthony He 1133985\n",
    "\n",
    "This part is adapted from the sample code provided"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/nhe/opt/anaconda3/lib/python3.9/site-packages (4.62.3)\r\n"
     ]
    }
   ],
   "source": [
    "# import packages (sorted alphabetically)\n",
    "!pip install tqdm # please skip this line if it is already installed\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from random import random\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# all files will be stored in the property_meta folder\n",
    "property_files = '../data/raw/property_meta'\n",
    "if not os.path.exists(property_files):\n",
    "    os.makedirs(property_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# read the scraped url\n",
    "# if the url has been scraped, then it will not be scraped again\n",
    "scraped_url = []\n",
    "if os.path.exists(f'{property_files}/property_url.csv'):\n",
    "    with open(f'{property_files}/property_url.csv', newline='') as inputfile:\n",
    "        for row in csv.reader(inputfile):\n",
    "            scraped_url.append(row[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set the header of the soup\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppelWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "sort_methods = [\"default-desc\", \"dateupdated-desc\", \"price-asc\", \"price-desc\", \"suburb-asc\"]\n",
    "N_PAGES = range(1, 51) # update this to your liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialise varaiables\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:48<00:00, 33.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# gather all links that should be scraped\n",
    "for sort_method in tqdm(sort_methods):\n",
    "    for page in N_PAGES:\n",
    "        url = BASE_URL + f\"/rent/melbourne-region-vic/?sort={sort_method}&page={page}\"\n",
    "        bs_object = BeautifulSoup(requests.get(url, headers = headers).text, \"html.parser\")\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "        index_links = bs_object.find(\"ul\",{\"data-testid\": \"results\"}).findAll(\"a\",href=re.compile(f\"{BASE_URL}/*\"))\n",
    "        for link in index_links:\n",
    "            # if its a property address, add it to the list\n",
    "            if 'address' in link['class']:\n",
    "                url_links.append(link['href'])\n",
    "        sleep(round(random(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "5323"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of urls\n",
    "len(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "870"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only retains the ones that were not scraped\n",
    "temp = []\n",
    "for i in url_links:\n",
    "    if not i in scraped_url:\n",
    "        temp.append(i)\n",
    "url_links = temp\n",
    "# check the number of urls that is going to be scraped in this run\n",
    "len(url_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url_links = list(set(url_links))\n",
    "num_url = len(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# urls are stored in a csv file\n",
    "# as property information are updated real time and scraped over a week\n",
    "# future scraping should be compared to this to avoid duplication\n",
    "with open(f'{property_files}/property_url.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for x in url_links + scraped_url:\n",
    "        csvwriter.writerow([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539/539 [04:29<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# for each url, scrape some basic metadata\n",
    "# this segment of code may need to be modified if domain.com.au makes any changes\n",
    "# this code is valid at the time of 12 September\n",
    "for property_url in tqdm(url_links):\n",
    "    bs_object = BeautifulSoup(requests.get(property_url, headers = headers).text, \"html.parser\")\n",
    "    # looks for the header class to get property name\n",
    "    property_metadata[property_url]['name'] = bs_object.find(\"h1\", {\"class\": \"css-164r41r\"}).text\n",
    "    property_metadata[property_url]['type'] = bs_object.find(\"div\", {\"data-testid\": \"listing-summary-property-type\"}).text\n",
    "    # looks for the div containing a summary title for cost\n",
    "    property_metadata[property_url]['cost_text'] = bs_object.find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}).text\n",
    "    # extract coordinates from the hyperlink provided\n",
    "    property_metadata[property_url]['coordinates'] = [\n",
    "        float(coord) for coord in re.findall(\n",
    "            r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "            bs_object\n",
    "                .find(\n",
    "                \"a\",\n",
    "                {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "            )\n",
    "                .attrs['href']\n",
    "        )[0].split(',')\n",
    "    ]\n",
    "    property_metadata[property_url]['rooms'] = [\n",
    "        re.findall(r'\\d\\s[A-Za-z]+', feature.text) for feature in bs_object\n",
    "            .find(\"div\", {\"data-testid\": \"property-features\"})\n",
    "            .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "    ]\n",
    "    property_metadata[property_url]['desc'] = re.sub(r'<br\\/>', '\\n', str(bs_object.find(\"p\"))).strip('</p>')\n",
    "    sleep(round(3*random(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the results of scraping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# merge json file with the data scraped this time\n",
    "if os.path.exists(f'{property_files}/property_metadata.json'):\n",
    "    with open(f'{property_files}/property_metadata.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    d = defaultdict(list, data)\n",
    "    for key, value in d.items():\n",
    "        for subkey, subvalue in value.items():\n",
    "            property_metadata[key][subkey] = subvalue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# write the new json file\n",
    "with open(f'{property_files}/property_metadata.json', 'w') as f:\n",
    "    json.dump(property_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# merge csv file with the data scraped this time\n",
    "if os.path.exists(f'{property_files}/property_metadata.csv'):\n",
    "    df = pd.read_csv(f'{property_files}/property_metadata.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# write the current scraped metadata into a Pandas dataframe to save them into a csv file\n",
    "df2 = pd.DataFrame(property_metadata).T.reset_index()\n",
    "df2 = df2.rename(columns = {'index':'url'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "if os.path.exists(f'{property_files}/property_metadata.csv'):\n",
    "    save_csv = pd.concat([df, df2])\n",
    "else:\n",
    "    save_csv = df2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_csv.to_csv(f'{property_files}/property_metadata.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}