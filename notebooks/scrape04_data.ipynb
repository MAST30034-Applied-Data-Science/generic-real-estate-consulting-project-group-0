{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all subrub\n",
    "\n",
    "import requests\n",
    "import html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from decimal import Decimal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request \n",
    "\n",
    "import random\n",
    "from random import uniform\n",
    "from random import sample\n",
    "\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from requests_ip_rotator import ApiGateway, EXTRA_REGIONS\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "from random import randrange\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME FUNCTIONS USED IN ALL SCRAPING#\n",
    "\n",
    "#Notes:\n",
    "\n",
    "# (1)\n",
    "\n",
    "# If get responses are non-success, try to open the website, manually,\n",
    "# on your own browser with your own unblocked internet/ip-address.\n",
    "# Inspect the page to get the request header that works.\n",
    "# Copy the headers into the get_header function,\n",
    "# but leave out your user agent and keep the 'User-Agent':sample(ua_l,1)[0].\n",
    "\n",
    "# (2)\n",
    "\n",
    "# At the time of original scraping, All of the links that is crawled and obtained by scrape03_all_link.ipynb\n",
    "# was sucessfully scraped by scrape04_data.ipynb. However, for the past week, A few of the links seems to \n",
    "# not be retrievable, even when the request header was reset as adviced at (1).\n",
    "\n",
    "# However, the nature of the failures seems to be random, for example: out of 30 pages of Hawthorn rent listing,\n",
    "# 2 of the pages is not retrievable, it is believed that the error occured on the host's side.\n",
    "\n",
    "# (3)\n",
    "\n",
    "# Always couple get_gateway() with gateway.shutdown() \n",
    "\n",
    "# (4)\n",
    "\n",
    "# time.sleep(uniform(1,2)) is set after every request get in getpage_aws_ip() for ethical reason\n",
    "# the sleep values can be changed at your own discretion\n",
    "\n",
    "\n",
    "# Obtaining a big list of user agent to be rotated in the get headers\n",
    "def get_useragent():\n",
    "\n",
    "    file_list=['android-browser.csv','chrome.csv','firefox.csv','internet-explorer.csv','opera.csv','safari.csv']\n",
    "\n",
    "    ua_l = [];\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        url = 'https://raw.githubusercontent.com/N0taN3rd/userAgentLists/master/csv/'+file;\n",
    "        df = pd.read_csv(url, index_col=0)\n",
    "        ua_l += df.index.to_list();\n",
    "        \n",
    "    return ua_l;\n",
    "\n",
    "# Create effective, user-like, headers using user agent rotator\n",
    "def get_header(ua_l):\n",
    "    \n",
    "    headers = { \n",
    "               'If-Modified-Since': 'Tue, 27 Sep 2022 01:26:05 GMT',\n",
    "                'If-None-Match': \"a3P8L3LUf+cFKEYW/6EIEKfHggI=\",\n",
    "                'User-Agent':sample(ua_l,1)[0]}\n",
    "    \n",
    "    return headers;\n",
    "\n",
    "# Opening AmazonAPI gateways to \n",
    "def get_gateway(base_url):\n",
    "    \n",
    "    AWS_ACCESS_KEY_ID = 'AKIAUKKMLM7SJVLB377D'\n",
    "    AWS_SECRET_ACCESS_KEY = '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'\n",
    "    \n",
    "    gateway = ApiGateway(site=base_url, regions=EXTRA_REGIONS, access_key_id = AWS_ACCESS_KEY_ID, access_key_secret = AWS_SECRET_ACCESS_KEY)\n",
    "    gateway.start()\n",
    "    \n",
    "    return gateway\n",
    "\n",
    "def getpage_aws_ip(gateway,base_url,site,ua_l):\n",
    "    \n",
    "    page_not_found = True;\n",
    "    \n",
    "    count =1;\n",
    "    \n",
    "    headers = get_header(ua_l);\n",
    "    session = requests.Session()\n",
    "    session.mount(base_url, gateway)\n",
    "    response = session.post(base_url+site,headers=headers);\n",
    "    time.sleep(uniform(1,2))\n",
    "    \n",
    "    # Unqote the code below if you want to apply some retries with rotated IP Address.\n",
    "    # Change the maximum count at your own discretion\n",
    "    '''\n",
    "    while response.ok == False and count <10:\n",
    "    \n",
    "        headers = get_header(ua_l);\n",
    "        session = requests.Session()\n",
    "        session.mount(base_url, gateway)\n",
    "        response = session.post(base_url+site,headers);\n",
    "        print(response)\n",
    "        time.sleep(uniform(1,2))\n",
    "        count =+1;\n",
    "    '''\n",
    "    \n",
    "    return response;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving functions\n",
    "\n",
    "def save_csv(path,columns,tpl):\n",
    "        \n",
    "    with open(path,'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(columns)\n",
    "        csv_out.writerow(tpl)\n",
    "        \n",
    "def update_csv(path,tpl):\n",
    "    with open(path,'a') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(tpl)\n",
    "            \n",
    "\n",
    "def save_list_json(unscrapped_l, path_str):\n",
    "    \n",
    "    with open(path_str, 'w') as myfile:\n",
    "        json.dump(unscrapped_l, myfile)\n",
    "        \n",
    "def get_list_json(path):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        l = json.load(f)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the collected links\n",
    "\n",
    "def get_data(ua_l,base_url):\n",
    "    \n",
    "    try:\n",
    "        data1 = get_list_json('../data/raw/unscrapped.json')\n",
    "        start_bool = False;\n",
    "    except:\n",
    "        data1 = get_list_json('../data/raw/all_links.json');\n",
    "        random.shuffle(data1)\n",
    "        start_bool = True;\n",
    "        \n",
    "    \n",
    "    gateway = get_gateway(base_url);\n",
    "\n",
    "    while data1 !=[]:\n",
    "        \n",
    "        print(str(len(data1))+' left');\n",
    "    \n",
    "        random_i = randrange(len(data1));\n",
    "        site_str = data1[random_i];\n",
    "        \n",
    "        print(base_url+site_str);\n",
    "    \n",
    "        suburb, post_code = re.findall(r'[a-zA-Z+]+/\\d{4}', site_str)[0].split('/');\n",
    "    \n",
    "        suburb = suburb.replace(\"+\", \" \")\n",
    "    \n",
    "        page = getpage_aws_ip(gateway,base_url,site_str,ua_l)\n",
    "    \n",
    "        #print(base_url+site_str);\n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'html.parser');\n",
    "    \n",
    "        prop_div_l= soup.find_all(\"div\", {\"class\":\\\n",
    "                                    [\"property odd clearfix\",\"property even clearfix\"]});\n",
    "        \n",
    "        datatpl_l = [];\n",
    "        \n",
    "        \n",
    "        for prop_div in prop_div_l: # for loop \n",
    "        \n",
    "        \n",
    "\n",
    "            prop_list = removeall_l_f(list(removeall_l_f(list(prop_div.children),' ')[0].children), ' ');\n",
    "        \n",
    "\n",
    "            prop_attribute_html = prop_list[0];\n",
    "            prop_attribute_list = removeall_l_f(list(prop_attribute_html), ' ');\n",
    "            \n",
    "            # Get the attribute of property such as Number of Bedroom and Number of Bathroom\n",
    "            \n",
    "            attribute_d =  get_attribute(prop_attribute_list)\n",
    "            \n",
    "            \n",
    "\n",
    "            prop_hstp_html = prop_list[2];\n",
    "            prop_hstp_list = list(list(prop_hstp_html)[3]);\n",
    "            prop_hstp_list = removeall_l_f(prop_hstp_list,' ');\n",
    "            #print(prop_hstp_list)\n",
    "\n",
    "            price_d = get_historical_price(prop_hstp_list);\n",
    "\n",
    "            n_row = len(price_d['year']);\n",
    "            \n",
    "            \n",
    "            for i in range(n_row):\n",
    "                \n",
    "                tpl = ( price_d['year'][i],\\\n",
    "                           price_d['month'][i],\\\n",
    "                           attribute_d['bed'][0],\\\n",
    "                           attribute_d['bath'][0],\\\n",
    "                           attribute_d['car'][0],\\\n",
    "                           attribute_d['land'][0],\\\n",
    "                           attribute_d['type'][0],\\\n",
    "                           attribute_d['address'],\\\n",
    "                           suburb,\\\n",
    "                           post_code,\\\n",
    "                           price_d['rent_raw'][i],\\\n",
    "                           base_url+site_str\n",
    "                          );\n",
    "                \n",
    "                \n",
    "                \n",
    "                #print(enc.encode(tpl));\n",
    "                \n",
    "                datatpl_l.append(tpl)\n",
    "                \n",
    "        data1.pop(random_i);\n",
    "        \n",
    "        \n",
    "        for tpl in datatpl_l:\n",
    "            \n",
    "            if start_bool == True:\n",
    "                columns = ['year','month','bed','bath','car',\\\n",
    "                           'land_raw','type','address', 'suburb',\\\n",
    "                           'code', 'rent_raw','url'];\n",
    "                save_csv('../data/raw/melbourne_past_listings.csv',columns,tpl)\n",
    "                \n",
    "                start_bool = False;\n",
    "                continue;\n",
    "            update_csv('../data/raw/melbourne_past_listings.csv',tpl)\n",
    "            \n",
    "        save_list_json(data1, '../data/raw/unscrapped.json');\n",
    "                         \n",
    "    gateway.shutdown()\n",
    "    \n",
    "    return data1\n",
    "\n",
    "def get_attribute(prop_attribute_list):\n",
    "    \n",
    "    data_d = {'bed':[],'bath':[],'car':[],'land':[],'type':[],'address':[]};\n",
    "\n",
    "    for i in range(0,len(prop_attribute_list)):\n",
    "        \n",
    "        # 'address':[]\n",
    "        \n",
    "        if prop_attribute_list[i].get('class')[0] == \"address\":\n",
    "            data_d['address'] = prop_attribute_list[i].text;\n",
    "            continue;\n",
    "    \n",
    "        if (prop_attribute_list[i].get('class')[0] != \"property-meta\"):\n",
    "            class_s = prop_attribute_list[i].get('class')[0];\n",
    "        else:\n",
    "            class_s = prop_attribute_list[i].get('class')[1];\n",
    "            \n",
    "        \n",
    "        data_s = prop_attribute_list[i].find(text=True, recursive=False);\n",
    "        \n",
    "        \n",
    "        #'type':[],\n",
    "        \n",
    "        if class_s == 'type':\n",
    "            \n",
    "            data_s = data_s.replace(' ','')\n",
    "            \n",
    "            \n",
    "            data_d[class_s].append(data_s);\n",
    "            continue;\n",
    "        #need to be preprocessed fruther;\n",
    "    \n",
    "        data_l = data_s.split(' ');\n",
    "        data_l = removeall_l_f(data_l,'')\n",
    "    \n",
    "        \n",
    "        #'land':[],\n",
    "    \n",
    "        if class_s=='land':\n",
    "            #land_bool = True;\n",
    "            #val = process_land(data_s);\n",
    "            data_d[class_s].append(data_s);\n",
    "            continue\n",
    "        \n",
    "        # 'bed':[],'bath':[],'car':[],\n",
    "        \n",
    "        try:\n",
    "            if land_bool == False:\n",
    "                val = float(data_l[0]);\n",
    "        except:\n",
    "            val = data_s;\n",
    "            \n",
    "    \n",
    "        try:\n",
    "            data_d[class_s].append(val);\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    data_d = nan_empty_val(data_d);\n",
    "            \n",
    "    return data_d\n",
    "\n",
    "def get_historical_price(prop_hstp_list):\n",
    "    data_d = {'month':[],'year':[],'rent_raw':[]};\n",
    "\n",
    "    weeks_in_year_i = 52;\n",
    "    weeks_in_month_i = 4.34524;\n",
    "    \n",
    "    na_i = 0;\n",
    "    nm_i = 0;\n",
    "    nw_i = 0;\n",
    "    \n",
    "    first_bool = False;\n",
    "    \n",
    "    for i in range(0,len(prop_hstp_list)):\n",
    "        \n",
    "        date_s = prop_hstp_list[i].find('span').text\n",
    "        data_s = prop_hstp_list[i].find(text=True, recursive=False);\n",
    "        [month_s,year_s] = date_s.split(' ');\n",
    "        \n",
    "        data_d['rent_raw'].append(data_s);\n",
    "        \n",
    "        \n",
    "        data_d['month'].append(month_s);\n",
    "        data_d['year'].append(year_s);\n",
    "\n",
    "        \n",
    "    return data_d\n",
    "\n",
    "def removeall_l_f(l,element):\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            l.remove(element)\n",
    "    except ValueError:\n",
    "        pass \n",
    "    \n",
    "    return l\n",
    "def nan_empty_val(d):\n",
    "    \n",
    "    for key in d:\n",
    "        \n",
    "        if d[key] == []:\n",
    "            d[key].append(float(\"nan\"))\n",
    "            \n",
    "    return d;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.oldlistings.com.au';   \n",
    "ua_l = get_useragent();\n",
    "get_data(ua_l,base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
